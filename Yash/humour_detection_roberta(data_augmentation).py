# -*- coding: utf-8 -*-
"""Humour Detection - Roberta(Data Augmentation).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/157CHxnLY6Iappj3Wo5seuSVl54afbuIQ

# Transformer Library by Hugging Faces and Torch 1.7.1 for using cuda
"""

!pip install -qq transformers
!pip install torch==1.7.1

"""# Necessary Imports

"""

# Commented out IPython magic to ensure Python compatibility.
import transformers
from transformers import RobertaModel, RobertaTokenizer, AdamW, get_linear_schedule_with_warmup
import torch
import os

import numpy as np
import pandas as pd
import seaborn as sns
from pylab import rcParams
import matplotlib.pyplot as plt
from matplotlib import rc
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from collections import defaultdict
from textwrap import wrap

from shutil import copyfile
from torch import nn, optim
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F

from sklearn.metrics import f1_score, accuracy_score

from google.colab import drive

# %matplotlib inline
# %config InlineBackend.figure_format='retina'

sns.set(style='whitegrid', palette='muted', font_scale=1.2)

HAPPY_COLORS_PALETTE = ["#01BEFE", "#FFDD00", "#FF7D00", "#FF006D", "#ADFF02", "#8F00FF"]

sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))

rcParams['figure.figsize'] = 12, 8

RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
device


drive.mount('/content/gdrive', force_remount=True)

"""# Data Exploration

Loading the dataset
"""

train_df = pd.read_csv('/content/train.csv')
dev_df = pd.read_csv('/content/dev.csv')
test_df = pd.read_csv('/content/test.csv')
aug_df = pd.read_csv('/content/filtered_augemented_data.csv')

"""Analyzing the data"""

train_df.shape

aug_df.shape

"""Adding augumented data to train_df"""

train_df.drop(['id','humor_rating','humor_controversy', 'offense_rating'] , inplace=True, axis=1)
train_df = pd.concat([train_df, aug_df])

"""Shape after adding the augmented data"""

train_df.shape

train_df.head()

train_df.info()

"""Count of each class in train data."""

class_names = ['Humorous', 'Non-Humorous']
ax = sns.countplot(train_df.is_humor)
plt.xlabel('Class Counts')
ax.set_xticklabels(class_names)

"""# Data Preprocessing"""

PRE_TRAINED_MODEL_NAME = 'roberta-base'

"""Loading a pre-trained RobertaTokenizer:"""

tokenizer = RobertaTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)

"""Analysing the token length for each entry of the train data."""

token_lens = []

for txt in train_df.text:
  tokens = tokenizer.encode(txt, max_length=512)
  token_lens.append(len(tokens))

"""Plot of token lengths"""

sns.distplot(token_lens)
plt.xlim([0, 256]);
plt.xlabel('Token count');

"""Most of the texts contains less than 100 tokens, so choosing a maximum length of 100."""

MAX_LEN = 100

"""Creating a custom dataset class."""

class CustomDataset(Dataset):

  def __init__(self, text, labels, tokenizer, max_len):
    self.text = text
    self.labels = labels
    self.tokenizer = tokenizer
    self.max_len = max_len
  
  def __len__(self):
    return len(self.text)
  
  def __getitem__(self, item):
    text = str(self.text[item])
    label = self.labels[item]

    encoding = self.tokenizer.encode_plus(
      text,
      add_special_tokens=True,
      max_length=self.max_len,
      return_token_type_ids=False,
      padding= 'max_length',
      return_attention_mask=True,
      return_tensors='pt',
      truncation= True
    )

    return {
      'text': text,
      'input_ids': encoding['input_ids'].flatten(),
      'attention_mask': encoding['attention_mask'].flatten(),
      'true_label': torch.tensor(label, dtype=torch.long)
    }

""" For computing and storing the average and current value"""

class AverageMeter(object):
    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count

"""Helper function for data loader"""

def create_data_loader(df, tokenizer, max_len, batch_size):
  ds = CustomDataset(
    text=df.text.to_numpy(),
    labels=df.is_humor.to_numpy(),
    tokenizer=tokenizer,
    max_len=max_len
  )

  return DataLoader(
    ds,
    batch_size=batch_size,
    num_workers=16
  )

"""Loading data from the data loader"""

BATCH_SIZE = 32

train_data_loader = create_data_loader(train_df, tokenizer, MAX_LEN, BATCH_SIZE)
val_data_loader = create_data_loader(dev_df, tokenizer, MAX_LEN, BATCH_SIZE)
test_data_loader = create_data_loader(test_df, tokenizer, MAX_LEN, BATCH_SIZE)

"""# Humour Detection with Roberta

Loading the Roberta  Model.
"""

roberta_model = RobertaModel.from_pretrained(PRE_TRAINED_MODEL_NAME)

"""Classifier on top of roberta"""

class MyClassifier(nn.Module):

  def __init__(self, n_classes):
    super(MyClassifier, self).__init__()
    self.roberta = RobertaModel.from_pretrained(PRE_TRAINED_MODEL_NAME, return_dict=False)
    self.drop = nn.Dropout(p=0.3)
    self.out = nn.Linear(self.roberta.config.hidden_size, n_classes)
  
  def forward(self, input_ids, attention_mask):
    _, pooled_output = self.roberta(
      input_ids=input_ids,
      attention_mask=attention_mask
    )
    output = self.drop(pooled_output)
    return self.out(output)

model = MyClassifier(2)
model = model.to(device)

"""### Training

Train parameters
"""

EPOCHS = 5

optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)
total_steps = len(train_data_loader) * EPOCHS

scheduler = get_linear_schedule_with_warmup(
  optimizer,
  num_warmup_steps=0,
  num_training_steps=total_steps
)

loss_fn = nn.CrossEntropyLoss().to(device)

"""Helper function for training model for one epoch"""

def train_epoch(
  model, 
  data_loader, 
  loss_fn, 
  optimizer, 
  device, 
  scheduler, 
  n_examples
):
  model = model.train()
  loss_avg = AverageMeter()

  losses = []
  correct_predictions = 0
  
  for d in data_loader:
    input_ids = d["input_ids"].to(device)
    attention_mask = d["attention_mask"].to(device)
    targets = d["true_label"].to(device)

    outputs = model(
      input_ids=input_ids,
      attention_mask=attention_mask
    )

    t = targets.float().view(-1,1) #[32,1]
    
    loss = loss_fn(outputs[:, 0:2], t[:, 0].long())

    loss.backward()
    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
    optimizer.step()
    scheduler.step()
    optimizer.zero_grad()

    loss_avg.update(loss.item(), targets.shape[0])

  return loss_avg.avg

"""Helper function to evaluate the model."""

def eval_model(model, data_loader, loss_fn, device, n_examples):
  model = model.eval()

  loss_avg = AverageMeter()
  o_   = torch.tensor([]).to(device) #use for metrics
  t_   = torch.tensor([]).to(device)

  with torch.no_grad():
    for d in data_loader:
      input_ids = d["input_ids"].to(device)
      attention_mask = d["attention_mask"].to(device)
      targets = d["true_label"].to(device)

      outputs = model(
        input_ids=input_ids,
        attention_mask=attention_mask
      )

      t = targets.float().view(-1,1) #[128,1] batch size
      loss = loss_fn(outputs[:, 0:2], t[:, 0].long())

      # EVALUATE
      max_o = torch.argmax(outputs[:, 0:2], dim=1).view(-1,1)
      max_o = max_o.detach()
      o_    = torch.cat((o_, max_o)  , 0) #float and append
      t_    = torch.cat((t_, targets), 0)
      loss_avg.update(loss.item(), targets.shape[0])

    o_ = o_.cpu().numpy()
    t_ = t_.view(-1,1)# change shape same as o_
    t_ = t_.cpu().numpy()
    fscore = f1_score(t_[:, 0], o_[:, 0])
    accuracy= accuracy_score(t_[:, 0], o_[:, 0])

    return loss_avg.avg, [fscore, accuracy]

"""Training Loop"""

train_loss_arr = []
val_loss_arr = []
val_metrics = [] #[f1-score , accuracy]

for epoch in range(EPOCHS):

  print(f'Epoch {epoch + 1}/{EPOCHS}')
  print('-' * 10)

  train_loss = train_epoch(
    model,
    train_data_loader,    
    loss_fn, 
    optimizer, 
    device, 
    scheduler, 
    len(train_df)
  )
  print("Training Loss : ", train_loss)
  
  val_loss, metrics = eval_model(
    model,
    val_data_loader,
    loss_fn,
    device,
    len(dev_df)
  )
  print("Validation Loss : ", val_loss)
  train_loss_arr.append(train_loss)
  val_loss_arr.append(val_loss)
  val_metrics.append(metrics)

plt.plot(train_loss_arr, "--o", label='train loss')
plt.plot(val_loss_arr, "--o", label='validation loss')

plt.title('Training history')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend()
plt.ylim([0, 1])

plt.plot([pt[0] for pt in val_metrics], "--o", label='Validation F1-score')
plt.plot([pt[1] for pt in val_metrics], "--o", label='validation Accuracy')

plt.title('Validation history')
plt.xlabel('Epoch')
plt.legend()
plt.ylim([0, 1])

"""### Evaluation

"""

gold_test_loss, gold_metrics = eval_model(
  model,
  test_data_loader,
  loss_fn,
  device,
  len(test_df)
)

print("Gold test Loss : ", gold_test_loss)
print("Gold test F1-Score : ", gold_metrics[0])
print("Gold test Accuracy : ", gold_metrics[1])

"""### Saving the model"""

run_iter = 5
save_dir = os.path.join('my_model_weights', 'run_{}'.format(run_iter))

if not os.path.exists(save_dir):
  print("Creating directory {}".format(save_dir))
  os.makedirs(save_dir)

save_path = os.path.join(save_dir, "epoch_{}".format(EPOCHS) + ".pth")
state = {
			'model': model.cpu().state_dict(),
			'optimizer': optimizer.state_dict(),
			'epoch': EPOCHS,
		}

torch.save(state, save_path)
model = model.to(device)

"""Saving weights in drive"""

copyfile('/content/my_model_weights/run_5/epoch_5.pth', '/content/gdrive/MyDrive/Roberta_model_with_aug_weights.pth')

"""# LOADING TRAINED MODEL FILE TO EVALUATE THE MODEL """

def load_model_weights(model, path= None):
  m = torch.load(path, map_location= torch.device('cpu'))
  if 'state_dict' in m:
    sd = m['state_dict']
  else:
    sd = m['model']

  model_dict = model.state_dict()

  state_dict = {k: v for k, v in sd.items() if (k in model_dict) and (v.shape == model_dict[k].shape)}
  model_dict.update(state_dict) 
  model.load_state_dict(model_dict)

  if len(model_dict)!=len(state_dict):
      print("")
      
  start_epoch = m['epoch'] + 1
  return model, start_epoch

"""Load model and test on gold-test data"""

def test_after_loading_model():
  path = "/content/gdrive/MyDrive/Roberta_model_with_aug_weights.pth"
  model = MyClassifier(2)
  model, start_epoch = load_model_weights(model, path)
  model = model.to(device)

  gold_test_loss, gold_metrics = eval_model(
    model,
    test_data_loader,
    loss_fn,
    device,
    len(test_df)
  )

  print("Gold test Loss : ", gold_test_loss)
  print("Gold test F1-Score : ", gold_metrics[0])
  print("Gold test Accuracy : ", gold_metrics[1])

test_after_loading_model()

