{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lakehead_NLP_Humor_Detection_GloVe_LSTM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXeLsJxMqfLi",
        "outputId": "1b9ec526-0647-47c8-cbbb-7a3551c931f4"
      },
      "source": [
        "#Mount drive to access files in gdrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_D4sguNqgu8"
      },
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, Input\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGETVONzqrt8"
      },
      "source": [
        "# reading semval datasets\n",
        "df_train = pd.read_csv(\"/content/gdrive/MyDrive/Lakehead_NLP_Project/train.csv\")\n",
        "df_test = pd.read_csv(\"/content/gdrive/MyDrive/Lakehead_NLP_Project/dev.csv\")\n",
        "df_gold = pd.read_csv(\"/content/gdrive/MyDrive/Lakehead_NLP_Project/gold-test-27446.csv\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euk_CunNquii",
        "outputId": "eefd91b0-4c9a-4b07-dedb-745408a53b7a"
      },
      "source": [
        "# text preprocessing\n",
        "%%time\n",
        "def text_preprocessing(text):\n",
        "  text = text.lower()\n",
        "  text = re.sub(r\"http\\S+\", \" \", text)\n",
        "  text = re.sub(' +', ' ', text)\n",
        "  \n",
        "  return text"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
            "Wall time: 6.2 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXtp-NcVy3gl"
      },
      "source": [
        "#applying text preprocessing on all dataset\n",
        "\n",
        "# train data\n",
        "df_train_1 = df_train.copy()\n",
        "df_train_1['text'] = df_train_1['text'].apply(text_preprocessing)\n",
        "\n",
        "#test data\n",
        "df_test_1 = df_test.copy()\n",
        "df_test_1['text'] = df_test_1['text'].apply(text_preprocessing)\n",
        "\n",
        "#gold data\n",
        "df_gold_1 = df_gold.copy()\n",
        "df_gold_1['text'] = df_gold_1['text'].apply(text_preprocessing)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKTgBOKtirWZ"
      },
      "source": [
        "# intializing tokenizer and fitting on train, test and dev data and padding to each sequences\n",
        "\n",
        "vocabulary_size = 20000\n",
        "maxlen = 200\n",
        "\n",
        "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
        "\n",
        "#creating vocabulary on train data\n",
        "tokenizer.fit_on_texts(df_train_1['text'])\n",
        "\n",
        "#generating sequence of tokens on train, text, gold data\n",
        "train_sequences = tokenizer.texts_to_sequences(df_train_1['text'])\n",
        "test_sequences = tokenizer.texts_to_sequences(df_test_1['text'])\n",
        "gold_sequences = tokenizer.texts_to_sequences(df_gold_1['text'])\n",
        "\n",
        "#padding the sequences\n",
        "train_pad = pad_sequences(train_sequences, maxlen=maxlen)\n",
        "test_pad = pad_sequences(test_sequences, maxlen=maxlen)\n",
        "gold_pad = pad_sequences(gold_sequences, maxlen=maxlen)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-j0jnwerWKv"
      },
      "source": [
        "# reading glove embedding and creating weight matrix \n",
        "def read_glove_embedding(vocabulary_size, df, glove_path, embedding_size, tokenizer):\n",
        "\n",
        "  embeddings_index = dict()\n",
        "\n",
        "  f = open(glove_path)\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = coefs\n",
        "  f.close()\n",
        "  print('Loaded %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "  embedding_matrix = np.zeros((vocabulary_size, embedding_size))\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "      if index > vocabulary_size - 1:\n",
        "          break\n",
        "      else:\n",
        "          embedding_vector = embeddings_index.get(word)\n",
        "          if embedding_vector is not None:\n",
        "              embedding_matrix[index] = embedding_vector\n",
        "\n",
        "  return embedding_matrix\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUKX9twetWJq",
        "outputId": "a4d1dfdb-7208-445d-af2e-98eb601f1b16"
      },
      "source": [
        "#create embedding for train data\n",
        "vocabulary_size = 20000\n",
        "df_new = df_train_1\n",
        "glove_path = '/content/gdrive/MyDrive/Lakehead_NLP_Project/glove.6B.300d.txt'\n",
        "embedding_size = 300\n",
        "\n",
        "embedding_matrix = read_glove_embedding(vocabulary_size, df_new, glove_path,  embedding_size, tokenizer)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yUXqWkOyBwJ"
      },
      "source": [
        "# defining metrices to evaluate the model\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_CtZsUKtKp-"
      },
      "source": [
        "# constructing model\n",
        "\n",
        "embedding_input_shape = 300\n",
        "model_glove = Sequential()\n",
        "model_glove.add(Embedding(vocabulary_size, embedding_input_shape, input_length=150, weights=[embedding_matrix], trainable=False))\n",
        "model_glove.add(Dropout(0.2))\n",
        "model_glove.add(Conv1D(64, 5, activation='relu'))\n",
        "model_glove.add(MaxPooling1D(pool_size=4))\n",
        "model_glove.add(LSTM(300))\n",
        "model_glove.add(Dense(1, activation='sigmoid'))\n",
        "model_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy',f1_m, precision_m, recall_m])\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qI2t5jw8tPxD",
        "outputId": "3a3e49b4-5791-4b65-8cd1-e6d368777423"
      },
      "source": [
        "%%time\n",
        "#fitting model on train data\n",
        "model_glove.fit(train_pad, np.array(df_train_1['is_humor']), validation_data=(test_pad, np.asarray(df_test_1['is_humor'])), epochs = 10, batch_size=128)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 150) for input KerasTensor(type_spec=TensorSpec(shape=(None, 150), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None, 200).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 150) for input KerasTensor(type_spec=TensorSpec(shape=(None, 150), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None, 200).\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.5819 - binary_accuracy: 0.6715 - f1_m: 0.7336 - precision_m: 0.6483 - recall_m: 0.8612WARNING:tensorflow:Model was constructed with shape (None, 150) for input KerasTensor(type_spec=TensorSpec(shape=(None, 150), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None, 200).\n",
            "63/63 [==============================] - 53s 52ms/step - loss: 0.5806 - binary_accuracy: 0.6727 - f1_m: 0.7347 - precision_m: 0.6499 - recall_m: 0.8616 - val_loss: 0.4406 - val_binary_accuracy: 0.7840 - val_f1_m: 0.8378 - val_precision_m: 0.7883 - val_recall_m: 0.8954\n",
            "Epoch 2/10\n",
            "63/63 [==============================] - 2s 38ms/step - loss: 0.3494 - binary_accuracy: 0.8400 - f1_m: 0.8740 - precision_m: 0.8578 - recall_m: 0.8960 - val_loss: 0.4229 - val_binary_accuracy: 0.8040 - val_f1_m: 0.8531 - val_precision_m: 0.7995 - val_recall_m: 0.9162\n",
            "Epoch 3/10\n",
            "63/63 [==============================] - 2s 35ms/step - loss: 0.2734 - binary_accuracy: 0.8861 - f1_m: 0.9071 - precision_m: 0.9067 - recall_m: 0.9121 - val_loss: 0.3825 - val_binary_accuracy: 0.8110 - val_f1_m: 0.8511 - val_precision_m: 0.8370 - val_recall_m: 0.8680\n",
            "Epoch 4/10\n",
            "63/63 [==============================] - 2s 35ms/step - loss: 0.2153 - binary_accuracy: 0.9134 - f1_m: 0.9283 - precision_m: 0.9319 - recall_m: 0.9271 - val_loss: 0.3546 - val_binary_accuracy: 0.8310 - val_f1_m: 0.8664 - val_precision_m: 0.8539 - val_recall_m: 0.8815\n",
            "Epoch 5/10\n",
            "63/63 [==============================] - 2s 35ms/step - loss: 0.1566 - binary_accuracy: 0.9368 - f1_m: 0.9483 - precision_m: 0.9454 - recall_m: 0.9530 - val_loss: 0.3892 - val_binary_accuracy: 0.8330 - val_f1_m: 0.8578 - val_precision_m: 0.9046 - val_recall_m: 0.8164\n",
            "Epoch 6/10\n",
            "63/63 [==============================] - 2s 35ms/step - loss: 0.1050 - binary_accuracy: 0.9603 - f1_m: 0.9681 - precision_m: 0.9706 - recall_m: 0.9662 - val_loss: 0.4368 - val_binary_accuracy: 0.8240 - val_f1_m: 0.8616 - val_precision_m: 0.8457 - val_recall_m: 0.8801\n",
            "Epoch 7/10\n",
            "63/63 [==============================] - 2s 36ms/step - loss: 0.0857 - binary_accuracy: 0.9689 - f1_m: 0.9747 - precision_m: 0.9746 - recall_m: 0.9757 - val_loss: 0.5978 - val_binary_accuracy: 0.8160 - val_f1_m: 0.8597 - val_precision_m: 0.8226 - val_recall_m: 0.9026\n",
            "Epoch 8/10\n",
            "63/63 [==============================] - 2s 36ms/step - loss: 0.0469 - binary_accuracy: 0.9841 - f1_m: 0.9870 - precision_m: 0.9873 - recall_m: 0.9869 - val_loss: 0.6666 - val_binary_accuracy: 0.7910 - val_f1_m: 0.8453 - val_precision_m: 0.7846 - val_recall_m: 0.9192\n",
            "Epoch 9/10\n",
            "63/63 [==============================] - 2s 35ms/step - loss: 0.0804 - binary_accuracy: 0.9693 - f1_m: 0.9753 - precision_m: 0.9719 - recall_m: 0.9802 - val_loss: 0.4903 - val_binary_accuracy: 0.8270 - val_f1_m: 0.8641 - val_precision_m: 0.8477 - val_recall_m: 0.8823\n",
            "Epoch 10/10\n",
            "63/63 [==============================] - 2s 36ms/step - loss: 0.0554 - binary_accuracy: 0.9791 - f1_m: 0.9829 - precision_m: 0.9836 - recall_m: 0.9827 - val_loss: 0.4598 - val_binary_accuracy: 0.8380 - val_f1_m: 0.8668 - val_precision_m: 0.8981 - val_recall_m: 0.8392\n",
            "CPU times: user 38.6 s, sys: 1.86 s, total: 40.5 s\n",
            "Wall time: 1min 41s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7a54ced2d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1KGeOiJunWY",
        "outputId": "7b28366a-694f-46d3-8eb0-fb89568b1d06"
      },
      "source": [
        "# evaluating model on gold data\n",
        "gold_lstm_results = model_glove.evaluate(gold_pad, np.asarray(df_gold_1['is_humor']), verbose=0, batch_size=128)\n",
        "\n",
        "print(f'Gold Data accuracy: {gold_lstm_results[1]*100:0.2f}')\n",
        "print(f'Gold Data F1 Score: {gold_lstm_results[2]}')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gold Data accuracy: 81.80\n",
            "Gold Data F1 Score: 0.8452799916267395\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DH75QA11Z8NT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}