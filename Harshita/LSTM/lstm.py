# -*- coding: utf-8 -*-
"""LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZMR1FKU8GUY0x4nZt-25jyvSFdjy3rty
"""

#Import Libraries
import pandas as pd
import nltk as nltk
import numpy
from nltk.corpus import stopwords
import keras
import re
import string
from keras.utils.vis_utils import plot_model
from keras import backend as K
from keras.models import Sequential, Model
from keras.layers import Dense, LSTM, Input, Activation, Dropout
from keras.optimizers import RMSprop, Adam
from sklearn.feature_extraction.text import CountVectorizer
from keras.layers.embeddings import Embedding

nltk.download('stopwords')

#Mount drive to access files in gdrive
from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)

# Read Train, Test and Gold dataset
df_train = pd.read_csv("/content/gdrive/MyDrive/NLP_Project/train.csv")
df_test = pd.read_csv("/content/gdrive/MyDrive/NLP_Project/dev.csv")
df_gold = pd.read_csv("/content/gdrive/MyDrive/NLP_Project/gold-test.csv")

# Preprocessing data lowercase and removing hyperlinks and stopwords
stop_words = set(stopwords.words('english'))  
def preProcessData(data_frame):
    data_frame['text'] = (data_frame['text']).str.lower()
    data_frame['text'] = data_frame['text'].apply(lambda x: re.sub(r"^https?:\/\/.*[\r\n]*", "", x, flags=re.MULTILINE))
    data_frame['text'] = data_frame['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))
    return data_frame

# Creating train, test and gold labels
X_train = preProcessData(df_train)['text']
X_test = preProcessData(df_test)['text']
X_gold = preProcessData(df_gold)['text']
Y_train = df_train['is_humor']
Y_test = df_test['is_humor']
Y_gold = df_gold['is_humor']

# Bag-of-words vectorization
cv = CountVectorizer() 
vect = cv.fit(X_train)
Bagofwords_train = vect.transform(X_train).toarray()
Bagofwords_test = cv.transform(X_test).toarray()
Bagofwords_gold = cv.transform(X_gold).toarray()

# Metrics Calculation

def recall_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())
    print(recall)
    return recall
# calculating precision
def precision_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    print('precesion', precision)
    return precision
# calculating F1 Score
def f1_m(y_true, y_pred):
    precision = precision_m(y_true, y_pred)
    recall = recall_m(y_true, y_pred)
    return 2*((precision*recall)/(precision+recall+K.epsilon()))

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Initializing Model 
# model = Sequential()
# model.add(Embedding(100000, 100, input_length=50, trainable=False))
# model.add(LSTM(100))
# model.add(Dense(128, activation='relu'))
# model.add(Dropout(0.2))
# model.add(Dense(1, activation='sigmoid'))
# model.summary()
# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy',f1_m, precision_m, recall_m])

# Commented out IPython magic to ensure Python compatibility.
# # Training the model
# %%time
# model.fit(Bagofwords_train,  Y_train, epochs=7, batch_size= 128)

# Plotting model
plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)

# Evaluating Model
test_accuracy = model.evaluate(Bagofwords_test, Y_test)
gold_accuracy = model.evaluate(Bagofwords_gold, Y_gold)

print('Test set\n  Loss: {:0.3f}\n  Accuracy: {:0.3f}\n F1 Score:{:0.3f}'.format(test_accuracy[0],test_accuracy[1], test_accuracy[2]))
print('Gold set\n  Loss: {:0.3f}\n  Accuracy: {:0.3f}\n F1 Score:{:0.3f}'.format(gold_accuracy[0],gold_accuracy[1], gold_accuracy[2]))